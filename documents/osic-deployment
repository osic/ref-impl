OSIC Deployment Process
=======================

Table of Contents
-----------------

-  `Provisioning the Deployment
   Host <https://github.com/osic/ref-impl/blob/master/documents/bare_metal_provisioning.md#provisioning-the-deployment-host>`__
-  `Download and Setup the osic-prep LXC
   Container <https://github.com/osic/ref-impl/blob/master/documents/bare_metal_provisioning.md#download-and-setup-the-osic-prep-lxc-container>`__
-  `PXE Boot the
   Servers <https://github.com/osic/ref-impl/blob/master/documents/bare_metal_provisioning.md#pxe-boot-the-servers>`__
-  `Bootstrapping the
   Servers <https://github.com/osic/ref-impl/blob/master/documents/bare_metal_provisioning.md#bootstrapping-the-servers>`__
-  `Create the osic-prep LXC
   Container <https://github.com/osic/ref-impl/blob/master/documents/bare_metal_provisioning.md#create-the-osic-prep-lxc-container>`__

Provisioning the Deployment Host
--------------------------------

You have been allocated a certain number of bare metal servers. There is
currently nothing running on these servers. You will need to manually
provision the first host. This will become your deployment host that
will be used to provision the rest of the servers using PXE.

Manually Provision the Deployment Host
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

First, download a `modified Ubuntu Server 14.04.3
ISO <http://public.thornelabs.net/ubuntu-14.04.3-server-i40e-hp-raid-x86_64.iso>`__.
The modified Ubuntu Server ISO contains i40e driver version 1.3.47 and
HP iLO tools.

Boot the deployment host to this ISO using a USB drive, CD/DVD-ROM,
iDRAC, or iLO. Whatever is easiest.

**NOTE:** to get an access to a server console through ILO, simply look
for the host ILO ip address through a web browser, login with the
credentials provided and then you can request a remote console from the
GUI.

**Before you move on, be sure you have unselected or removed the ISO
from the ILO console** so that future server reboots do not continue to
use it to boot.

Once the deployment host is booted to the ISO, follow these steps to
begin installation:

1. Select **Language**

2. Hit **Fn + F6**

3. Dismiss the **Expert mode** menu by hiting **Esc**.

4. Scroll to the beginning of the line and delete
   **file=/cdrom/preseed/ubuntu-server.seed**.

5. Type
   **preseed/url=https://github.com/osic/ref-impl/blob/master/files/ubuntu-server-14.04.seed**

6. Hit **Enter** to begin the install process.

7. You will be prompted for the following menus:

-  Select a language
-  Select your location
-  Configure the keyboard
-  Configure the network

DHCP detection will fail. You will need to manually select the proper
network interface - typically **p1p1** - and manually configure
networking on the **PXE** network (refer to your onboarding email to
find the **PXE** network information). When asked for name servers, type
8.8.8.8 8.8.4.4.

You may see an error stating: "/dev/sda" contains GPT signatures,
indicating that it had a GPT table... Is this a GPT partition table? If
you encounter this error select "No" and continue.

Once networking is configured, the Preseed file will be downloaded. The
remainder of the Ubuntu install will be unattended.

The Ubuntu install will be finished when the system reboots and a login
prompt appears.

Update Linux Kernel
~~~~~~~~~~~~~~~~~~~

Once the system boots, it can be SSH'd to using the IP address you
manually assigned. Login with username **root** and password
**cobbler**.

You will need to update the Linux kernel on the deployment host in order
to get an updated upstream i40e driver.

::

    apt-get update; apt-get install -y linux-generic-lts-xenial

When the update finishes running, reboot the server and proceed with the
rest of the guide.

Download and Setup the osic-prep LXC Container
----------------------------------------------

With the deployment host provisioning done, SSH to it.

Next, you will download a pre-packaged LXC container that contains
everything you need to PXE boot the rest of the servers.

Setup LXC Linux Bridge
~~~~~~~~~~~~~~~~~~~~~~

In order to use the LXC container, a new bridge will need to be created:
**br-pxe**.

**NOTE: Follow these instructions very carefully.**

First, install the necessary packages:

::

    apt-get install vlan bridge-utils

Reconfigure the network interface file **/etc/network/interfaces** to
match the following (your IP addresses and ports will most likely be
different):

::

    # The loopback network interface
    auto lo
    iface lo inet loopback

    auto p1p1
    iface p1p1 inet manual

    # Container Bridge
    auto br-pxe
    iface br-pxe inet static
    address 172.22.0.21
    netmask 255.255.252.0
    gateway 172.22.0.1
    dns-nameservers 8.8.8.8 8.8.4.4
    bridge_ports p1p1
    bridge_stp off
    bridge_waitport 0
    bridge_fd 0

Bring up **br-pxe**. I recommend you have access to the iLO in case the
following commands fail and you lose network connectivity:

::

    ifdown p1p1; ifup br-pxe
    
    Install LXC and Configure LXC Container
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Install the necessary LXC package:

::

    apt-get install lxc

Change into root's home directory:

::

    cd /root

Download the LXC container to the deployment host:

::

    wget http://23.253.105.87/osic.tar.gz

Untar the LXC container:

::

    tar xvzf /root/osic.tar.gz

Move the LXC container directory into the proper directory:

::

    mv /root/osic-prep /var/lib/lxc/

Once moved, the LXC container should be stopped, verify by running
``lxc-ls -f``. Before starting it, open
**/var/lib/lxc/osic-prep/config** and change **lxc.network.ipv4 =
172.22.0.22/22** to a valid IP address from the PXE network you are
using. Do not forget to set the CIDR notation as well. If your PXE
network already is **172.22.0.22/22**, you do not need to make further
changes.

::

    lxc.network.type = veth
    lxc.network.name = eth1
    lxc.network.ipv4 = 172.22.0.22/22
    lxc.network.link = br-pxe
    lxc.network.hwaddr = 00:16:3e:xx:xx:xx
    lxc.network.flags = up
    lxc.network.mtu = 1500

Start the LXC container:

::

    lxc-start -d --name osic-prep

You should be able to ping the IP address you just set for the LXC
container from the host.

Configure LXC Container
~~~~~~~~~~~~~~~~~~~~~~~

There are a few configuration changes that need to be made to the
pre-packaged LXC container for it to function on your network.

Start by attaching to the LXC container:

::

    lxc-attach --name osic-prep

If you had to change the IP address above, reconfigure the DHCP server
by running the following sed commands. You will need to change
**172.22.0.22** to match the IP address you set above:

::

    sed -i '/^next_server: / s/ .*/ 172.22.0.22/' /etc/cobbler/settings

    sed -i '/^server: / s/ .*/ 172.22.0.22/' /etc/cobbler/settings

Open **/etc/cobbler/dhcp.template** and reconfigure your DHCP settings.
You will need to change the **subnet**, **netmask**, **option routers**,
**option subnet-mask**, and **range dynamic-bootp** parameters to match
your network.

::

    subnet 172.22.0.0 netmask 255.255.252.0 {
         option routers             172.22.0.1;
         option domain-name-servers 8.8.8.8;
         option subnet-mask         255.255.252.0;
         range dynamic-bootp        172.22.0.23 172.22.0.200;
         default-lease-time         21600;
         max-lease-time             43200;
         next-server                $next_server;

Finally, restart Cobbler and sync it:

::

    service cobbler restart

    cobbler sync

At this point you can PXE boot any servers, but it is still a manual
process. In order for it to be an automated process, a CSV file needs to
be created.

PXE Boot the Servers
--------------------

Gather MAC Addresses
~~~~~~~~~~~~~~~~~~~~

Go to root home directory

::

    cd /root

You will need to obtain the MAC address of the network interface (e.g.
p1p1) configured to PXE boot on every server. Be sure the MAC addresses
are mapped to their respective hostname.

You can do this by logging into the LXC container and creating a CSV
file named **ilo.csv**. Use the information from your onboarding email
to create the CSV.

For example:

::

    729427-controller01,10.15.243.158,controller
    729426-controller02,10.15.243.157,controller
    729425-controller03,10.15.243.156,controller
    729424-logging01,10.15.243.155,logging
    729423-logging02,10.15.243.154,logging
    729422-logging03,10.15.243.153,logging
    729421-compute01,10.15.243.152,compute
    729420-compute02,10.15.243.151,compute
    729419-compute03,10.15.243.150,compute
    729418-compute04,10.15.243.149,compute
    729417-compute05,10.15.243.148,compute
    729416-compute06,10.15.243.147,compute
    729415-compute07,10.15.243.146,compute
    729414-compute08,10.15.243.145,compute
    729413-cinder01,10.15.243.144,cinder
    729412-cinder02,10.15.243.143,cinder
    729411-cinder03,10.15.243.142,cinder
    729410-swift01,10.15.243.141,swift
    729409-swift02,10.15.243.140,swift
    729408-swift03,10.15.243.139,swift

Be sure to remove any spaces in your CSV file. We also recommend
removing the deployment host you manually provisioned from this CSV so
you do not accidentally reboot the host you are working from.

Once this information is collected, it will be used to create another
CSV file that will be the input for many different steps in the build
process.

Create Input CSV
~~~~~~~~~~~~~~~~

Now, we will use a script to create a CSV named **input.csv** in the
following format.

::

    hostname,mac-address,host-ip,host-netmask,host-gateway,dns,pxe-interface,cobbler-profile

If this will be an openstack-ansible installation, it is recommended to
order the rows in the CSV file in the following order, otherwise order
the rows however you wish:

1. Controller nodes
2. Logging nodes
3. Compute nodes
4. Cinder nodes
5. Swift nodes

An example for openstack-ansible installations:

::

    744800-infra01.example.com,A0:36:9F:7F:70:C0,10.240.0.51,255.255.252.0,10.240.0.1,8.8.8.8,p1p1,ubuntu-14.04.3-server-unattended-osic-generic
    744819-infra02.example.com,A0:36:9F:7F:6A:C8,10.240.0.52,255.255.252.0,10.240.0.1,8.8.8.8,p1p1,ubuntu-14.04.3-server-unattended-osic-generic
    744820-infra03.example.com,A0:36:9F:82:8C:E8,10.240.0.53,255.255.252.0,10.240.0.1,8.8.8.8,p1p1,ubuntu-14.04.3-server-unattended-osic-generic
    744821-logging01.example.com,A0:36:9F:82:8C:E9,10.240.0.54,255.255.252.0,10.240.0.1,8.8.8.8,p1p1,ubuntu-14.04.3-server-unattended-osic-generic
    744822-compute01.example.com,A0:36:9F:82:8C:EA,10.240.0.55,255.255.252.0,10.240.0.1,8.8.8.8,p1p1,ubuntu-14.04.3-server-unattended-osic-generic
    744823-compute02.example.com,A0:36:9F:82:8C:EB,10.240.0.56,255.255.252.0,10.240.0.1,8.8.8.8,p1p1,ubuntu-14.04.3-server-unattended-osic-generic
    744824-cinder01.example.com,A0:36:9F:82:8C:EC,10.240.0.57,255.255.252.0,10.240.0.1,8.8.8.8,p1p1,ubuntu-14.04.3-server-unattended-osic-cinder
    744825-object01.example.com,A0:36:9F:7F:70:C1,10.240.0.58,255.255.252.0,10.240.0.1,8.8.8.8,p1p1,ubuntu-14.04.3-server-unattended-osic-swift
    744826-object02.example.com,A0:36:9F:7F:6A:C2,10.240.0.59,255.255.252.0,10.240.0.1,8.8.8.8,p1p1,ubuntu-14.04.3-server-unattended-osic-swift
    744827-object03.example.com,A0:36:9F:82:8C:E3,10.240.0.60,255.255.252.0,10.240.0.1,8.8.8.8,p1p1,ubuntu-14.04.3-server-unattended-osic-swift

To do just that, the following command will loop through each iLO IP
address in **ilo.csv** to obtain the MAC address of the network
interface configured to PXE boot and setup rest of information as well
as shown above:

**NOTE:** make sure to Set COUNT to the first usable address after
deployment host and container (ex. If you use .2 and .3 for deployment
and container, start with .4 controller1).

::

    COUNT=23
    for i in $(cat ilo.csv)
    do
        NAME=`echo $i | cut -d',' -f1`
        IP=`echo $i | cut -d',' -f2`
        TYPE=`echo $i | cut -d',' -f3`

        case "$TYPE" in
          cinder)
                SEED='ubuntu-14.04.3-server-unattended-osic-cinder'
                ;;
            swift)
                SEED='ubuntu-14.04.3-server-unattended-osic-swift'
                ;;
            *)
            SEED='ubuntu-14.04.3-server-unattended-osic-generic'
                ;;
        esac
        MAC=`sshpass -p calvincalvin ssh -o StrictHostKeyChecking=no root@$IP show /system1/network1/Integrated_NICs | grep Port1 | cut -d'=' -f2`
        #hostname,mac-address,host-ip,host-netmask,host-gateway,dns,pxe-interface,cobbler-profile
        echo "$NAME,${MAC//[$'\t\r\n ']},172.22.0.$COUNT,255.255.252.0,172.22.0.1,8.8.8.8,p1p1,$SEED" | tee -a input.csv

        (( COUNT++ ))
    done

Assigning a Cobbler Profile
~~~~~~~~~~~~~~~~~~~~~~~~~~~

The last column in the CSV file specifies which Cobbler Profile to map
the Cobbler System to. You have the following options:

-  ubuntu-14.04.3-server-unattended-osic-generic
-  ubuntu-14.04.3-server-unattended-osic-generic-ssd
-  ubuntu-14.04.3-server-unattended-osic-cinder
-  ubuntu-14.04.3-server-unattended-osic-cinder-ssd
-  ubuntu-14.04.3-server-unattended-osic-swift
-  ubuntu-14.04.3-server-unattended-osic-swift-ssd

Typically, you will use the
**ubuntu-14.04.3-server-unattended-osic-generic** Cobbler Profile. It
will create one RAID10 raid group. The operating system will see this as
**/dev/sda**.

The **ubuntu-14.04.3-server-unattended-osic-cinder** Cobbler Profile
will create one RAID1 raid group and a second RAID10 raid group. These
will be seen by the operating system as **/dev/sda** and **/dev/sdb**,
respectively.

The **ubuntu-14.04.3-server-unattended-osic-swift** Cobbler Profile will
create one RAID1 raid group and 10 RAID0 raid groups each containing one
disk. The HP Storage Controller will not present a disk to the operating
system unless it is in a RAID group. Because Swift needs to deal with
individual, non-RAIDed disks, the only way to do this is to put each
disk in its own RAID0 raid group.

You will only use the **ssd** Cobbler Profiles if the servers contain
SSD drives.

Generate Cobbler Systems
~~~~~~~~~~~~~~~~~~~~~~~~

With this CSV file in place, run the **generate\_cobbler\_systems.py**
script to generate a **cobbler system** command for each server and pipe
the output to ``bash`` to actually add the **cobbler system** to
Cobbler:

::

    cd /root/rpc-prep-scripts

    python generate_cobbler_system.py /root/input.csv | bash

Verify the **cobbler system** entries were added by running
``cobbler system list``.

Once all of the **cobbler systems** are setup, run ``cobbler sync``.

Begin PXE Booting
~~~~~~~~~~~~~~~~~

To begin PXE booting, reboot all of the servers with the following
command (if the deployment host is the first controller, you will want
to **remove** it from the **ilo.csv** file so you don't reboot the host
running the LXC container):

::

    for i in $(cat /root/ilo.csv)
    do
    NAME=$(echo $i | cut -d',' -f1)
    IP=$(echo $i | cut -d',' -f2)
    echo $NAME
    ipmitool -I lanplus -H $IP -U root -P calvincalvin power reset
    done

**NOTE:** if the servers are already shut down, you might want to change
**power reset** with **power on** in the above command.

As the servers finish PXE booting, a call will be made to the cobbler
API to ensure the server does not PXE boot again.

To quickly see which servers are still set to PXE boot, run the
following command:

::

    for i in $(cobbler system list)
    do
    NETBOOT=$(cobbler system report --name $i | awk '/^Netboot/ {print $NF}')
    if [[ ${NETBOOT} == True ]]; then
    echo -e "$i: netboot_enabled : ${NETBOOT}"
    fi
    done

Any server which returns **True** has not yet PXE booted.

**NOTE**: In case you want to re-pxeboot servers, make sure to clean old
settings from cobbler with the following command:

::

    for i in `cobbler system list`; do cobbler system remove --name $i; done;

Bootstrapping the Servers
-------------------------

With the servers PXE booted, you will now need to bootstrap the servers.

Generate Ansible Inventory
~~~~~~~~~~~~~~~~~~~~~~~~~~

Start by running the ``generate_ansible_hosts.py`` Python script:

::

    cd /root/rpc-prep-scripts

    python generate_ansible_hosts.py /root/input.csv > /root/osic-prep-ansible/hosts

If this will be an openstack-ansible installation, organize the Ansible
**hosts** file into groups for **controller**, **logging**, **compute**,
**cinder**, and **swift**, otherwise leave the Ansible **hosts** file as
it is and jump to the next section.

An example for openstack-ansible or RPC-O installations:

::

    [controller]
    744800-infra01.example.com ansible_ssh_host=10.240.0.51
    744819-infra02.example.com ansible_ssh_host=10.240.0.52
    744820-infra03.example.com ansible_ssh_host=10.240.0.53

    [logging]
    744821-logging01.example.com ansible_ssh_host=10.240.0.54

    [compute]
    744822-compute01.example.com ansible_ssh_host=10.240.0.55
    744823-compute02.example.com ansible_ssh_host=10.240.0.56

    [cinder]
    744824-cinder01.example.com ansible_ssh_host=10.240.0.57

    [swift]
    744825-object01.example.com ansible_ssh_host=10.240.0.58
    744826-object02.example.com ansible_ssh_host=10.240.0.59
    744827-object03.example.com ansible_ssh_host=10.240.0.60

Verify Connectivity
~~~~~~~~~~~~~~~~~~~

The LXC container will not have all of the new server's SSH fingerprints
in its **known\_hosts** file. Programatically add them by running the
following command:

::

    for i in $(cat /root/osic-prep-ansible/hosts | awk /ansible_ssh_host/ | cut -d'=' -f2)
    do
    ssh-keygen -R $i
    ssh-keyscan -H $i >> /root/.ssh/known_hosts
    done

Verify Ansible can talk to every server (the password is **cobbler**):

::

    cd /root/osic-prep-ansible

    ansible -i hosts all -m shell -a "uptime" --ask-pass

Setup SSH Public Keys
~~~~~~~~~~~~~~~~~~~~~

Generate an SSH key pair for the LXC container:

::

    ssh-keygen

Copy the LXC container's SSH public key to the **osic-prep-ansible**
directory:

::

    cp /root/.ssh/id_rsa.pub /root/osic-prep-ansible/playbooks/files/public_keys/osic-prep

Bootstrap the Servers
~~~~~~~~~~~~~~~~~~~~~

Finally, run the bootstrap.yml Ansible Playbook:

::

    cd /root/osic-prep-ansible

    ansible-playbook -i hosts playbooks/bootstrap.yml --ask-pass

Clean Up LVM Logical Volumes
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If this will be an openstack-ansible installation, you will need to
clean up particular LVM Logical Volumes.

Each server is provisioned with a standard set of LVM Logical Volumes.
Not all servers need all of the LVM Logical Volumes. Clean them up with
the following steps.

Remove LVM Logical Volume **nova00** from the Controller, Logging,
Cinder, and Swift nodes:

::

    ansible-playbook -i hosts playbooks/remove-lvs-nova00.yml

Remove LVM Logical Volume **deleteme00** from all nodes:

::

    ansible-playbook -i hosts playbooks/remove-lvs-deleteme00.yml

Update Linux Kernel
~~~~~~~~~~~~~~~~~~~

Every server in the OSIC RAX Cluster is running two Intel X710 10 GbE
NICs. These NICs have not been well tested in Ubuntu and as such the
upstream i40e driver in the default 14.04.3 Linux kernel will begin
showing issues when you setup VLAN tagged interfaces and bridges.

In order to get around this, you must install an updated Linux kernel.

You can quickly do this by running the following commands:

::

    cd /root/osic-prep-ansible

    ansible -i hosts all -m shell -a "apt-get update; apt-get install -y linux-generic-lts-xenial" --forks 25

Reboot Nodes
~~~~~~~~~~~~

Finally, reboot all servers:

::

    ansible -i hosts all -m shell -a "reboot" --forks 25

Once all servers reboot, you can begin installing openstack-ansible.
